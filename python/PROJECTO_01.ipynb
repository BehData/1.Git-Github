{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FTTxniuovVo8"
   },
   "source": [
    "# Welcome to Beautiful Soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JgBv1sr5vtWK",
    "tags": []
   },
   "source": [
    "This notebook has been made as an example for us to understand a little bit of [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) capabilities. Beautiful Soup is a Python library built to pull data from HTML and XML files, these are codes that have a lot of inconsistency through web page design, and are very difficult to interact with unless previously parsed by a third party like this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Exercise\n",
    "\n",
    "The main goal of the exercise was to create a program that can crawl through web spaces and extract a sample of the data in their code. For this goal we used an example web page: [Known by Fikret](http://py4e-data.dr-chuck.net/known_by_Fikret.html)\n",
    "\n",
    "To add a little bit of spice to the exercise, we are also trying to enter the links in the page, update them and finally make a list with the results. The exact indication was \"Enter the third link in the page, repeat this process 4 more times\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "    # To run this, download the BeautifulSoup zip file\n",
    "    # http://www.py4e.com/code3/bs4.zip\n",
    "    # and unzip it in the same directory as this file\n",
    "\n",
    "    import urllib.request, urllib.parse, urllib.error\n",
    "    from bs4 import BeautifulSoup\n",
    "    import ssl\n",
    "    import re\n",
    "\n",
    "    # Ignore SSL certificate errors\n",
    "    ctx = ssl.create_default_context()\n",
    "    ctx.check_hostname = False\n",
    "    ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "    # Variables for the program\n",
    "    url = input('Enter - ')\n",
    "    cuenta = 0\n",
    "    nombres = list()\n",
    "\n",
    "    # Iterative repeater\n",
    "    while cuenta < 8:\n",
    "        slate = list()\n",
    "        html = urllib.request.urlopen(url, context=ctx).read()\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "        tags = soup('a')\n",
    "        # RegEx to find the selected text in the url\n",
    "        nombres.append(re.findall('[by][_](.+)[.]',url))\n",
    "\n",
    "        # Retrieve all of the anchor tags\n",
    "        for tag in tags:\n",
    "            # Append in the slate\n",
    "            slate.append(tag.get('href', None))\n",
    "\n",
    "        # Update variables\n",
    "        cuenta = cuenta + 1\n",
    "        url = slate[17]\n",
    "        continue\n",
    "\n",
    "    print(nombres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code output looks something like this (copy and paste this URL for prompt):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter -  http://py4e-data.dr-chuck.net/known_by_Fikret.html\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Fikret'], ['Margaret'], ['Ailee'], ['Banan'], ['Kealcy'], ['Mykie'], ['Lucy'], ['Archibald']]\n"
     ]
    }
   ],
   "source": [
    "# To run this, download the BeautifulSoup zip file\n",
    "# http://www.py4e.com/code3/bs4.zip\n",
    "# and unzip it in the same directory as this file\n",
    "\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import ssl\n",
    "import re\n",
    "\n",
    "# Ignore SSL certificate errors\n",
    "ctx = ssl.create_default_context()\n",
    "ctx.check_hostname = False\n",
    "ctx.verify_mode = ssl.CERT_NONE\n",
    "\n",
    "# Variables for the program\n",
    "url = input('Enter - ')\n",
    "cuenta = 0\n",
    "nombres = list()\n",
    "\n",
    "# Iterative repeater\n",
    "while cuenta < 8:\n",
    "    slate = list()\n",
    "    html = urllib.request.urlopen(url, context=ctx).read()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    tags = soup('a')\n",
    "    # RegEx to find the selected text in the url\n",
    "    nombres.append(re.findall('[by][_](.+)[.]',url))\n",
    "\n",
    "    # Retrieve all of the anchor tags\n",
    "    for tag in tags:\n",
    "        # Append in the slate\n",
    "        slate.append(tag.get('href', None))\n",
    "    \n",
    "    # Update variables\n",
    "    cuenta = cuenta + 1\n",
    "    url = slate[17]\n",
    "    continue\n",
    "\n",
    "print(nombres)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- Using Python to access web data (MOOC Course by Dr. Chuck): https://www.coursera.org/learn/python-network-data/\n",
    "- Known by Fikret (part of the course): http://py4e-data.dr-chuck.net/known_by_Fikret.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PROJECTO_01.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
